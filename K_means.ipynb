{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba as jb\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>octob inform camp refuge migrant calai known j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>catalyst trust major initi join iie creat plat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman candid want improv chanc elect grab seat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>octob argu loss octob local gyumri vanadzor se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>term servic end user licens agreement agreemen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cluster\n",
       "0  octob inform camp refuge migrant calai known j...        0\n",
       "1  catalyst trust major initi join iie creat plat...        0\n",
       "2  woman candid want improv chanc elect grab seat...        0\n",
       "3  octob argu loss octob local gyumri vanadzor se...        0\n",
       "4  term servic end user licens agreement agreemen...        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('twitter_big_long.json', lines=True)\n",
    "stopwords = open('stopword.txt', mode='r', encoding='utf-8').read().split('\\n')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                    text  cluster\n",
       "0     octob inform camp refuge migrant calai known j...        0\n",
       "1     catalyst trust major initi join iie creat plat...        0\n",
       "2     woman candid want improv chanc elect grab seat...        0\n",
       "3     octob argu loss octob local gyumri vanadzor se...        0\n",
       "4     term servic end user licens agreement agreemen...        0\n",
       "...                                                 ...      ...\n",
       "1495  submit ingrid carlqvist gateston institut perp...        5\n",
       "1496  ahead major solidar march london weekend bob d...        5\n",
       "1497  poster mock child refuge displai wall commun k...        5\n",
       "1498  jame joyc rescu migrant nautic mile tripoli li...        5\n",
       "1499  bahn bildet vier der lassen sich demnach zum e...        5\n",
       "\n",
       "[1500 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 5], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [octob, inform, camp, refuge, migrant, calai, ...\n",
       "1    [catalyst, trust, major, initi, join, iie, cre...\n",
       "2    [woman, candid, want, improv, chanc, elect, gr...\n",
       "3    [octob, argu, loss, octob, local, gyumri, vana...\n",
       "4    [term, servic, end, user, licens, agreement, a...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words = data['text'].apply(lambda x:str(x).split())\n",
    "data_words.head()\n",
    "# print(type(data_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_fre(X):\n",
    "    n = X.shape[0]\n",
    "    data_set = []\n",
    "    for i in range(n):\n",
    "        words = X.values[i]\n",
    "        words_set = {}\n",
    "        for word in words:\n",
    "            if word not in stopwords: # 去停用词\n",
    "                data_set.append(word)\n",
    "                if word in words_set.keys():\n",
    "                    words_set[word] += 1\n",
    "                else:\n",
    "                    words_set[word] = 1\n",
    "        X.values[i] = words_set\n",
    "    return X, set(data_set)\n",
    "        # print(type(X.values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'octob': 2, 'inform': 3, 'camp': 2, 'refuge':...\n",
       "1    {'catalyst': 7, 'trust': 6, 'major': 1, 'initi...\n",
       "2    {'woman': 2, 'candid': 2, 'improv': 1, 'chanc'...\n",
       "3    {'octob': 2, 'argu': 1, 'loss': 1, 'local': 1,...\n",
       "4    {'term': 22, 'servic': 20, 'end': 1, 'user': 2...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fre, data_set= word_fre(data_words)\n",
    "data_fre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个词的idf\n",
    "def idf(data_set, X):\n",
    "    data_idf = {i: 0 for i in data_set}\n",
    "    for i in data_set:\n",
    "        for j in range(X.shape[0]):\n",
    "            if i in X.values[j]:\n",
    "                data_idf[i] += 1\n",
    "        data_idf[i] = math.log(X.shape[0] / (data_idf[i] + 1))\n",
    "    return data_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算文档中每个词的词频tf\n",
    "def tf(X, data_fre):\n",
    "    for i in range(X.shape[0]):\n",
    "        n = len(X.values[i])\n",
    "        for j in data_fre.values[i]:\n",
    "            data_fre.values[i][j] = data_fre.values[i][j] / n\n",
    "    return data_fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'octob': 0.012738853503184714, 'inform': 0.01...\n",
      "1    {'catalyst': 0.02364864864864865, 'trust': 0.0...\n",
      "2    {'woman': 0.058823529411764705, 'candid': 0.05...\n",
      "3    {'octob': 0.046511627906976744, 'argu': 0.0232...\n",
      "4    {'term': 0.04573804573804574, 'servic': 0.0415...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_tf = tf(data_words, data_fre)\n",
    "print(data_tf.head())\n",
    "data_idf = idf(data_set, data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算tf * idf\n",
    "def tf_idf(x_tf, x_idf):\n",
    "    n = x_tf.shape[0]\n",
    "    for i in range(n):\n",
    "        for word in x_tf.values[i]:\n",
    "            x_tf.values[i][word] = x_tf.values[i][word] * x_idf[word]\n",
    "        sorted(x_tf.values[i].items(), key=operator.itemgetter(1),reverse=True) # 按照tf*idf的值降序排列\n",
    "    return x_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf_idf = tf_idf(data_tf, data_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       {'octob': 0.03155335643488947, 'inform': 0.034...\n",
       "1       {'catalyst': 0.13057508927376935, 'trust': 0.0...\n",
       "2       {'woman': 0.12092500088603059, 'candid': 0.151...\n",
       "3       {'octob': 0.11520644093668946, 'argu': 0.04879...\n",
       "4       {'term': 0.09308175886260968, 'servic': 0.0740...\n",
       "                              ...                        \n",
       "1495    {'submit': 0.008742177918843358, 'ingrid': 0.0...\n",
       "1496    {'ahead': 0.012137590890147688, 'major': 0.014...\n",
       "1497    {'poster': 0.08355805229024695, 'mock': 0.0440...\n",
       "1498    {'jame': 0.17283070808965334, 'joyc': 0.241022...\n",
       "1499    {'bahn': 0.3478085617551021, 'bildet': 0.04226...\n",
       "Name: text, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [] # 权重矩阵\n",
    "word = []   # 文本矩阵\n",
    "for i in range(data_tf_idf.shape[0]):\n",
    "    w1 = []\n",
    "    w2 = []\n",
    "    # 取前10个为特征项\n",
    "    c = 0\n",
    "    for j in data_tf_idf.values[i]:\n",
    "        if c == 10:\n",
    "            break\n",
    "        w1.append(data_tf_idf.values[i][j])\n",
    "        w2.append(j)\n",
    "        c += 1\n",
    "    weight.append(w1)\n",
    "    word.append(w2)\n",
    "    \n",
    "# print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算向量的距离\n",
    "def getEuclidean(point1, point2):\n",
    "    dimension = len(point1)\n",
    "    dist = 0.0\n",
    "    for i in range(dimension):\n",
    "        dist += (point1[i] - point2[i]) ** 2\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_means 实现\n",
    "import random\n",
    "def k_means(dataset, k, n_iter):\n",
    "    # 初始化k个质心向量\n",
    "    index = random.sample(list(range(len(dataset))), k) # 随机选择 k 个下标\n",
    "    vectors = []\n",
    "    for i in index:\n",
    "        vectors.append(dataset[i])\n",
    "    \n",
    "    # 初始化标签\n",
    "    labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        labels.append(-1)\n",
    "    \n",
    "    C = []\n",
    "    \n",
    "    # 重复迭代\n",
    "    while n_iter > 0:\n",
    "        C = []\n",
    "        for i in range(k):\n",
    "            C.append([])\n",
    "            \n",
    "        for id, val in enumerate(dataset): # 遍历所有的dataset，离质心最近的归为质心一类\n",
    "            classlabel = -1\n",
    "            min_dist = 1e6\n",
    "            for i, point in enumerate(vectors):\n",
    "                dist = getEuclidean(val, point)\n",
    "                if dist < min_dist:\n",
    "                    classlabel = i\n",
    "                    min_dist = dist\n",
    "            C[classlabel].append(val)\n",
    "            labels[id] = classlabel\n",
    "        \n",
    "        # 重新计算质心\n",
    "        for i, cluster in enumerate(C):\n",
    "            clusterheart = []\n",
    "            dimension = len(dataset[0])\n",
    "            for j in range(dimension):\n",
    "                clusterheart.append(0)\n",
    "            for item in cluster:\n",
    "                for j, val in enumerate(item):\n",
    "                    clusterheart[j] += val / len(cluster)\n",
    "            vectors[i] = clusterheart\n",
    "        \n",
    "        n_iter -= 1\n",
    "    return C, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, C_labels = k_means(weight, 6, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 2, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(C_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
      "       n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "       random_state=None, tol=0.0001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# 进行k-means用sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "clf = KMeans(n_clusters=6, max_iter=1000) # n_clusters 簇类个数， max_iter最大迭代数 \n",
    "s = clf.fit(weight)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 5, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true = data['cluster'].to_list()\n",
    "labels_pred = list(clf.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006469772183506995\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "rand_index_score1 = metrics.adjusted_rand_score(labels_true, C_labels) # 实现函数k_means 的兰德指数\n",
    "print(rand_index_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011942025428728605\n"
     ]
    }
   ],
   "source": [
    "rand_index_score2 = metrics.adjusted_rand_score(labels_true, labels_pred) # 调用 sklearn 的兰德指数\n",
    "print(rand_index_score2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
