{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\JupyterNotebook\\\\Numpy_practice\\\\第四次作业'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import codecs\n",
    "import os\n",
    "import random\n",
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>体育</td>\n",
       "      <td>麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>体育</td>\n",
       "      <td>黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>体育</td>\n",
       "      <td>双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体育</td>\n",
       "      <td>兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0    体育  鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...\n",
       "1    体育  麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...\n",
       "2    体育  黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...\n",
       "3    体育  双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...\n",
       "4    体育  兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = open('stopword.txt', encoding='utf-8').read().split('\\n')\n",
    "df = pd.read_csv('chinese_news.csv')\n",
    "df.head()\n",
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def text2tokens(raw_text):\n",
    "    tokens = jieba.lcut(raw_text) # jieba.cut(raw_text) 后直接生成list\n",
    "    tokens = [t for t in tokens if len(t) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "时政    1000\n",
       "房产    1000\n",
       "体育    1000\n",
       "娱乐    1000\n",
       "游戏    1000\n",
       "时尚    1000\n",
       "家居    1000\n",
       "科技    1000\n",
       "财经    1000\n",
       "教育    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\OYL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.881 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "documents = [text2tokens(t) for t in df['content']]\n",
    "# documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "    # 文档\n",
    "    def __init__(self):\n",
    "        self.words = []\n",
    "        self.length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreProcessing(object):\n",
    "    # 数据处理为词袋\n",
    "    def __init__(self):\n",
    "        self.docs_count = 0\n",
    "        self.words_count = 0\n",
    "        self.docs = []\n",
    "        self.word2id = OrderedDict()\n",
    "    \n",
    "    def cachewordidmap(self): # 将字典保存在本地（like dictionary.dict）\n",
    "        with codecs.open(path + 'wordidmapfile', 'w+', 'utf-8') as f:\n",
    "            for word, id in self.word2id.items():\n",
    "                f.write(word + '\\t' + str(id) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def preprocessing(df):\n",
    "    dpre = DataPreProcessing()         # 数据处理后的对象\n",
    "    item_idx = 0                       # 将词用数字来代替在字典中的位置\n",
    "    for text in df['content']:\n",
    "        doc = Document()               # 一篇文档对象\n",
    "        temp = text2tokens(text)       # 分词\n",
    "        for item in temp:\n",
    "            if item in dpre.word2id:             # 词在词典中存在，将对应的index加入doc\n",
    "                doc.words.append(dpre.word2id[item])\n",
    "            else:                                      # 新增于词典中，将index加入doc\n",
    "                dpre.word2id[item] = item_idx\n",
    "                doc.words.append(item_idx)\n",
    "                item_idx += 1\n",
    "        doc.length = len(temp)\n",
    "        dpre.docs.append(doc)\n",
    "    dpre.docs_count = len(dpre.docs)  # 文档的数目 1e4\n",
    "    dpre.words_count = len(dpre.word2id) # 词汇的数目\n",
    "    \n",
    "    # dpre.cachewordidmap()\n",
    "    return dpre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAmodel(object):\n",
    "    def __init__(self, dpre):\n",
    "        self.dpre = dpre  # dpre 预处理的数据对象\n",
    "        self.K = K        # 主题个数\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.iter_times = iter_times       # 最大迭代次数\n",
    "        self.top_words_num = top_words_num # 特征词个数\n",
    "        \n",
    "        self.p = np.zeros(self.K)          # 临时变量\n",
    "        self.nw = np.zeros((self.dpre.words_count, self.K), dtype=\"int\") # nw，词Word在主题topic上的分布\n",
    "        self.nwsum = np.zeros(self.K, dtype=\"int\")                       # nwsum，各topic下词的总数\n",
    "        self.nd = np.zeros((self.dpre.docs_count, self.K))               # nd，每个doc中各个topic的词的总数...\n",
    "        self.ndsum = np.zeros(dpre.docs_count, dtype=\"int\")              # ndsum，每各doc中词的总数\n",
    "        \n",
    "        self.Z = np.array([[0 for y in range(dpre.docs[x].length)] for x in range(dpre.docs_count)]) \n",
    "        # M * doc.size()，文档中的主题分布\n",
    "        \n",
    "        # 随机先分配类型，为每个文档中的各个单词分配主题\n",
    "        for x in range(len(self.Z)):\n",
    "            self.ndsum[x] = self.dpre.docs[x].length\n",
    "            for y in range(self.dpre.docs[x].length):\n",
    "                topic = random.randint(0, self.K - 1)           # 在 0 - K 中随机分配主题\n",
    "                self.Z[x][y] = topic                            # 文档中词的主题分布\n",
    "                self.nw[self.dpre.docs[x].words[y]][topic] += 1 # 第x篇文章第y个单词 在该topic上的分布\n",
    "                self.nd[x][topic] += 1                          # 第x篇文章中属于topic的词数 + 1\n",
    "                self.nwsum[topic] += 1                          # topic下的词的数量 + 1\n",
    "        \n",
    "        self.theta = np.array([[0.0 for y in range(self.K)] for x in range(self.dpre.docs_count)]) # 初始化theta doc的topic分布\n",
    "        self.phi = np.array([[0.0 for y in range(self.dpre.words_count)] for x in range(self.K)])  # 初始化 phi topic下的Word分布\n",
    "    \n",
    "    def sampling(self, i, j):\n",
    "        # 更换主题 第i个doc ，第j 个word\n",
    "        topic = self.Z[i][j]\n",
    "        word = self.dpre.docs[i].words[j]\n",
    "        \n",
    "        # 相关统计调整\n",
    "        self.nw[word][topic] -= 1\n",
    "        self.nwsum[topic] -= 1\n",
    "        self.nd[i][topic] -= 1\n",
    "        self.ndsum[i] -= 1\n",
    "        \n",
    "        Vbeta = self.dpre.words_count * self.beta\n",
    "        Kalpha = self.K * self.alpha\n",
    "        self.p = (self.nw[word] + self.beta) / (self.nwsum + Vbeta) * (self.nd[i] + self.alpha) / (self.ndsum[i] + Kalpha)\n",
    "        \n",
    "        p = np.squeeze(np.asarray(self.p / np.sum(self.p)))  # 把shape中为1的维度去掉\n",
    "        topic = np.argmax(np.random.multinomial(1, p))       # 一次实验，浮点数序列，长度p\n",
    "\n",
    "        self.nw[word][topic] += 1\n",
    "        self.nwsum[topic] += 1\n",
    "        self.nd[i][topic] += 1\n",
    "        self.ndsum[i] += 1\n",
    "        return topic\n",
    "        \n",
    "    def est(self):\n",
    "        # 进行训练\n",
    "        for x in range(self.iter_times):\n",
    "            for i in range(self.dpre.docs_count):\n",
    "                for j in range(self.dpre.docs[i].length):\n",
    "                    topic = self.sampling(i, j)\n",
    "                    self.Z[i][j] = topic\n",
    "        self._theta()\n",
    "        self._phi()\n",
    "        self.save()\n",
    "        \n",
    "    def _theta(self):\n",
    "        for i in range(self.dpre.docs_count):\n",
    "            self.theta[i] = (self.nd[i] + self.alpha) / (self.ndsum[i] + self.K * self.alpha)\n",
    "    \n",
    "    def _alpha(self):\n",
    "        for i in range(self.K):\n",
    "            self.phi[i] = (self.nw.T[i] + self.beta) / (self.nwsum[i] + self.dpre.words_count * self.beta)\n",
    "            \n",
    "    def save(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "iter_times = 1000\n",
    "top_words_num = 10\n",
    "dpre = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(dpre.docs_count):\n",
    "    X = X + dpre.docs[i].words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDAmodel(dpre)\n",
    "lda.est()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
